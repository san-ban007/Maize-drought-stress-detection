{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szWwJmqPHZ-r"
   },
   "source": [
    "## Training the Vision Transformer on a Custom Dataset\n",
    "\n",
    "In this notebook, we are going to fine-tune a pre-trained Vision Transformer (which can be found from [Huggingface](https://github.com/huggingface/transformers)) on a Custom Dataset. For this notebook we will be using the Rock, Paper, Scissors dataset which can be found [here](https://public.roboflow.com/classification/rock-paper-scissors/1). This dataset is a collection of 2925 images images in 3 different classes. This tutorial is based on Huggingface's [Fine tuning the Vision Transformer on CIFAR 10 notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb).\n",
    "\n",
    "### Accompanying Blog Post\n",
    "\n",
    "We recommend that you follow along in this notebook while reading the blog post on [How to Train the HuggingFace Vision Transformer On a Custom Dataset](blog.roboflow.com/how-to-train-the-huggingface-vision-transformer-on-a-custom-dataset/) concurrently.\n",
    "\n",
    "\n",
    "We will prepare the data using [Roboflow's Preprocessing Tools](https://docs.roboflow.com/image-transformations/image-preprocessing), and train the model using this notebook. \n",
    "\n",
    "### Steps Covered in this Tutorial\n",
    "\n",
    "In this tutorial, we will walk through the steps required to train a Vision Transformer on your custom classification data.\n",
    "\n",
    "To train our image classifier we take the following steps:\n",
    "\n",
    "* Install Vision Transformer dependencies\n",
    "* Download custom Image Classification data using Roboflow\n",
    "* Use the Vision Transformer Feature Extractor\n",
    "* Run the Vision Transformer training procedure\n",
    "* Evaluate the Vision Transformer on a test image\n",
    "* Export the Vision Transformer model for future inference\n",
    "\n",
    "\n",
    "### **About**\n",
    "\n",
    "[Roboflow](https://roboflow.com) enables teams to deploy custom computer vision models quickly and accurately. Convert data from to annotation format, assess dataset health, preprocess, augment, and more. It's free for your first 1000 source images.\n",
    "\n",
    "**Looking for a vision model available via API without hassle? Try Roboflow Train.**\n",
    "\n",
    "![Roboflow Wordmark](https://i.imgur.com/dcLNMhV.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_7rNZfIf301"
   },
   "source": [
    "Let's start by installing the relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6MCTZFkFw6i6",
    "outputId": "a174e771-daae-47fc-c96b-be627e41b9b6"
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DSSAj4Os-Od"
   },
   "source": [
    "Next, convert the folder structure dataset into a PyTorch dataset format using PyTorch's ImageFolder dataset structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgn-a3NiSjqR"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_ds = torchvision.datasets.ImageFolder('/home/sbaner24/Maize/Soybean_data/11-46_aug/data/train_res/', transform=ToTensor())\n",
    "valid_ds = torchvision.datasets.ImageFolder('/home/sbaner24/Maize/Soybean_data/11-46_aug/test_inf_res/', transform=ToTensor())\n",
    "test_ds = torchvision.datasets.ImageFolder('/home/sbaner24/Maize/Soybean_data/11-46_aug/data/test_res/', transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyHJjDYLfoFy"
   },
   "source": [
    "## Define the Model\n",
    "\n",
    "Here we define the model.\n",
    "\n",
    "The model itself uses a linear layer on top of a pre-trained `ViTModel`. We place a linear layer on top of the last hidden state of the [CLS] token, which serves as a good representation of an entire image. We also add dropout for regularization.\n",
    "\n",
    "**Note:** The Vision Transformer pretrained model can be used as a regular PyTorch layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cGDrb1Q4ToLN"
   },
   "outputs": [],
   "source": [
    "from transformers import ViTModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ViTForImageClassification(nn.Module):\n",
    "    def __init__(self, num_labels=5):\n",
    "        super(ViTForImageClassification, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, pixel_values, labels):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        output = self.dropout(outputs.last_hidden_state[:,0])\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "          loss_fct = nn.CrossEntropyLoss()\n",
    "          loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        if loss is not None:\n",
    "          return logits, loss.item()\n",
    "        else:\n",
    "          return logits, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BePiLK-LtXuG"
   },
   "source": [
    "## Define the Model Parameters\n",
    "\n",
    "To train this model, we will train in 3 epochs, with a batch size of 10 and a learning rate of 2e-5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntGvS0_wUAxc"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 35\n",
    "LEARNING_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "At9H-QOStt8_"
   },
   "source": [
    "We will use the pretrained Vision Transformer feature extractor, an Adam Optimizer, and a Cross Entropy Loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168,
     "referenced_widgets": [
      "2efb606ae85043d2aec636657b8dc23c",
      "1b6366fa2570491f852ca70c8dd78a3b",
      "39b0a4ffae63427ba7e1b688e5a6d011",
      "12fcdc8b8f9c4c14836f67a8012d43f7",
      "1fd362b69c9d44f58239aeb5080705c6",
      "4a0ebf07f34641d5b059e0def5d92d41",
      "d7ae2eb9cdf24069ad682092497e0bfe",
      "c6a703307fa84008a1a651a0bde9aff9",
      "36a2eee270944e1ea01101031a0e430c",
      "1ab261bd567b47f593bef7559d7140e5",
      "f60180a0cd674bb2b8842f6f174613b1",
      "6f521b76233846d8a09ee7ab98f7ea03",
      "cf4f03a541d94ab29d4825910471cf27",
      "6715a58a5d564ebf8bf64b705cf6f76c",
      "495a2c6efc654ff29bbcbe1a7ba1e41e",
      "0b5c8cf671ca4cb09703ee9dde8766c9",
      "d54bd61599c740ce9afbd6dd0cdf3918",
      "6373a61866e8427899a8c56ac463fc88",
      "892ab87763ca4569b1c14c7a864f0327",
      "f9ccf45eaeae49cd9ab55c8524fd06f7",
      "598d0a9ff37e4051a77fe32e33c89d69",
      "1de61920e64e41c8ba8da72e7ef9e27d",
      "f69783a89ebf4887aa55388c5f936d3b",
      "a6023bb8327044c4a6b478656261ef7d"
     ]
    },
    "id": "RIyJr8EDtvlR",
    "outputId": "1e4b58b2-ade2-4d6f-858b-8578ae8ea736"
   },
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "# Define Model\n",
    "model = ViTForImageClassification(len(train_ds.classes))    \n",
    "# Feature Extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "# Adam Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# Cross Entropy Loss\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "# Use GPU if available  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "if torch.cuda.is_available():\n",
    "    model.cuda() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTZ-BP1yu1Us"
   },
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ql2T5PDUI1D",
    "outputId": "ea0f05df-577b-4e15-f18a-5775824e838d"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "print(\"Number of train samples: \", len(train_ds))\n",
    "print(\"Number of test samples: \", len(test_ds))\n",
    "print(\"Detected Classes are: \", train_ds.class_to_idx) \n",
    "\n",
    "train_loader = data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "test_loader  = data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4) \n",
    "\n",
    "# Train the model\n",
    "for epoch in range(EPOCHS):        \n",
    "  for step, (x, y) in enumerate(train_loader):\n",
    "    # Change input array into list with each batch being one element\n",
    "    x = np.split(np.squeeze(np.array(x)), BATCH_SIZE)\n",
    "    # Remove unecessary dimension\n",
    "    for index, array in enumerate(x):\n",
    "      x[index] = np.squeeze(array)\n",
    "    # Apply feature extractor, stack back into 1 tensor and then convert to tensor\n",
    "    x = torch.tensor(np.stack(feature_extractor(x)['pixel_values'], axis=0))\n",
    "    # Send to GPU if available\n",
    "    x, y  = x.to(device), y.to(device)\n",
    "    b_x = Variable(x)   # batch x (image)\n",
    "    b_y = Variable(y)   # batch y (target)\n",
    "    # Feed through model\n",
    "    output, loss = model(b_x, None)\n",
    "    # Calculate loss\n",
    "    if loss is None: \n",
    "      loss = loss_func(output, b_y)   \n",
    "      optimizer.zero_grad()           \n",
    "      loss.backward()                 \n",
    "      optimizer.step()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "      # Get the next batch for testing purposes\n",
    "      test = next(iter(test_loader))\n",
    "      test_x = test[0]\n",
    "      # Reshape and get feature matrices as needed\n",
    "      test_x = np.split(np.squeeze(np.array(test_x)), BATCH_SIZE)\n",
    "      for index, array in enumerate(test_x):\n",
    "        test_x[index] = np.squeeze(array)\n",
    "      test_x = torch.tensor(np.stack(feature_extractor(test_x)['pixel_values'], axis=0))\n",
    "      # Send to appropirate computing device\n",
    "      test_x = test_x.to(device)\n",
    "      test_y = test[1].to(device)\n",
    "      # Get output (+ respective class) and compare to target\n",
    "      test_output, loss = model(test_x, test_y)\n",
    "      test_output = test_output.argmax(1)\n",
    "      # Calculate Accuracy\n",
    "      accuracy = (test_output == test_y).sum().item() / BATCH_SIZE\n",
    "      print('Epoch: ', epoch, '| train loss: %.4f' % loss, '| val accuracy: %.2f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4S-BcSI2v88"
   },
   "outputs": [],
   "source": [
    "torch.save(model, '/home/sbaner24/Maize/Soybean_data/Output/model_run5_32.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rS706FbORDiO",
    "outputId": "74fb0815-f1dd-463c-b012-3cfa546cbd08"
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = '/home/sbaner24/Maize/Soybean_data/Output/model_run5_32.pt'\n",
    "model = torch.load(MODEL_PATH)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vit.encoder.layer[0].attention.forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWXvWiB-srBC"
   },
   "source": [
    "## Evaluate on a Test Image\n",
    "\n",
    "Finally, let's evaluate the model on a test image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "ZLv_xdYssuGO",
    "outputId": "734efe1c-371c-4714-be8a-a79f0eceaefd"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#valid_ds = torchvision.datasets.ImageFolder('/home/sbaner24/Maize/Atten_Deep_MIL/Data/Trial006/test_inf/y20m10d20/', transform=ToTensor())\n",
    "\n",
    "EVAL_BATCH = 1\n",
    "eval_loader  = data.DataLoader(valid_ds, batch_size=EVAL_BATCH, shuffle=True, num_workers=4) \n",
    "# Disable grad\n",
    "with torch.no_grad():\n",
    "    \n",
    "  inputs, target = next(iter(eval_loader))\n",
    "  # Reshape and get feature matrices as needed\n",
    "  print(inputs.shape)\n",
    "  inputs = inputs[0].permute(1, 2, 0)\n",
    "  # Save original Input\n",
    "  originalInput = inputs\n",
    "  for index, array in enumerate(inputs):\n",
    "    inputs[index] = np.squeeze(array)\n",
    "  inputs = torch.tensor(np.stack(feature_extractor(inputs)['pixel_values'], axis=0))\n",
    "\n",
    "  # Send to appropriate computing device\n",
    "  inputs = inputs.to(device)\n",
    "  target = target.to(device)\n",
    " \n",
    "  # Generate prediction\n",
    "  prediction, loss = model(inputs, target)\n",
    "    \n",
    "  # Predicted class value using argmax\n",
    "  predicted_class = np.argmax(prediction.cpu())\n",
    "  value_predicted = list(valid_ds.class_to_idx.keys())[list(valid_ds.class_to_idx.values()).index(predicted_class)]\n",
    "  value_target = list(valid_ds.class_to_idx.keys())[list(valid_ds.class_to_idx.values()).index(target)]\n",
    "        \n",
    "  # Show result\n",
    "  plt.imshow(originalInput)\n",
    "  plt.xlim(224,0)\n",
    "  plt.ylim(224,0)\n",
    "  plt.title(f'Prediction: {value_predicted} - Actual target: {value_target}')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted=[]\n",
    "gt=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step, (x, y) in enumerate(eval_loader):\n",
    "        inputs, target = next(iter(eval_loader))\n",
    "        # Reshape and get feature matrices as needed\n",
    "        print(inputs.shape)\n",
    "        inputs = inputs[0].permute(1, 2, 0)\n",
    "        # Save original Input\n",
    "        originalInput = inputs\n",
    "        for index, array in enumerate(inputs):\n",
    "            inputs[index] = np.squeeze(array)\n",
    "        inputs = torch.tensor(np.stack(feature_extractor(inputs)['pixel_values'], axis=0))\n",
    "\n",
    "        # Send to appropriate computing device\n",
    "        inputs = inputs.to(device)\n",
    "        target = target.to(device)\n",
    " \n",
    "        # Generate prediction\n",
    "        prediction, loss = model(inputs, target)\n",
    "    \n",
    "        # Predicted class value using argmax\n",
    "        predicted_class = np.argmax(prediction.cpu())\n",
    "        value_predicted = list(valid_ds.class_to_idx.keys())[list(valid_ds.class_to_idx.values()).index(predicted_class)]\n",
    "        value_target = list(valid_ds.class_to_idx.keys())[list(valid_ds.class_to_idx.values()).index(target)]\n",
    "        print(\"Predicted:\"+str(value_predicted)+\" Target: \"+str(value_target))\n",
    "        predicted.append(value_predicted)\n",
    "        gt.append(value_target)\n",
    "\n",
    "print(\"Gt Labels:\", gt)\n",
    "print(\"Predicted Labels:\",predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_list=[0 if i=='Plant2' else 1 for i in predicted]\n",
    "#gt_list=[0 if i=='Plant2' else 1 for i in gt]\n",
    "pred_list= [eval(i) for i in predicted]\n",
    "gt_list= [eval(i) for i in gt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import matrix\n",
    "\n",
    "dst='/home/sbaner24/Maize/Soybean_data/Output/'\n",
    "\n",
    "cm = confusion_matrix(gt_list, pred_list)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index = ['0','1','2','3','4'], \n",
    "                     #index = ['0','1'], \n",
    "                     columns = ['0','1','2','3','4'])\n",
    "                     #columns = ['0','1'])\n",
    "#Plotting the confusion matrix\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(cm_df, annot=True)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "acc=np.mean(np.equal(pred_list, gt_list))\n",
    "one_off=np.mean(np.greater_equal(np.asarray(pred_list),  np.asarray(gt_list) - 1) & np.less_equal(np.asarray(pred_list), np.asarray(gt_list) + 1))\n",
    "rmse=np.sqrt(np.mean((np.asarray(pred_list)-np.asarray(gt_list))**2))\n",
    "mae=np.mean(np.abs(np.asarray(pred_list)-np.asarray(gt_list)))\n",
    "plt.title('Accuracy = {:4.2f} |One off = {:4.2f} | RMSE = {:4.2f} | MAE = {:4.2f}'.format(acc,one_off,rmse,mae))\n",
    "#plt.title('Accuracy = {:4.2f} | RMSE = {:4.2f} | MAE = {:4.2f}'.format(acc,one_off,rmse,mae))\n",
    "plt.savefig(dst+'cm_model_run5'+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGVIM5jX2Z5b"
   },
   "source": [
    "## Save the Entire Model\n",
    "\n",
    "We can save the entire model as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeJ99BfV9QRo"
   },
   "source": [
    "## Export Trained Model\n",
    "\n",
    "Now that you have trained your custom vision transformer, you can export the trained model you have made here for inference on your device elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gFmfiH0Z3QJb",
    "outputId": "2351b3dd-4ced-4300-f13c-18c740b12a11"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "%cp /content/model.pt /content/gdrive/My\\ Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORZKpeH1REHP"
   },
   "source": [
    "## Use your Exported Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0b5c8cf671ca4cb09703ee9dde8766c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12fcdc8b8f9c4c14836f67a8012d43f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c6a703307fa84008a1a651a0bde9aff9",
      "placeholder": "​",
      "style": "IPY_MODEL_d7ae2eb9cdf24069ad682092497e0bfe",
      "value": " 425/425 [00:23&lt;00:00, 18.1B/s]"
     }
    },
    "1ab261bd567b47f593bef7559d7140e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b6366fa2570491f852ca70c8dd78a3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1de61920e64e41c8ba8da72e7ef9e27d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1fd362b69c9d44f58239aeb5080705c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "2efb606ae85043d2aec636657b8dc23c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_39b0a4ffae63427ba7e1b688e5a6d011",
       "IPY_MODEL_12fcdc8b8f9c4c14836f67a8012d43f7"
      ],
      "layout": "IPY_MODEL_1b6366fa2570491f852ca70c8dd78a3b"
     }
    },
    "36a2eee270944e1ea01101031a0e430c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f60180a0cd674bb2b8842f6f174613b1",
       "IPY_MODEL_6f521b76233846d8a09ee7ab98f7ea03"
      ],
      "layout": "IPY_MODEL_1ab261bd567b47f593bef7559d7140e5"
     }
    },
    "39b0a4ffae63427ba7e1b688e5a6d011": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a0ebf07f34641d5b059e0def5d92d41",
      "max": 425,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1fd362b69c9d44f58239aeb5080705c6",
      "value": 425
     }
    },
    "495a2c6efc654ff29bbcbe1a7ba1e41e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4a0ebf07f34641d5b059e0def5d92d41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "598d0a9ff37e4051a77fe32e33c89d69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "6373a61866e8427899a8c56ac463fc88": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6715a58a5d564ebf8bf64b705cf6f76c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f521b76233846d8a09ee7ab98f7ea03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b5c8cf671ca4cb09703ee9dde8766c9",
      "placeholder": "​",
      "style": "IPY_MODEL_495a2c6efc654ff29bbcbe1a7ba1e41e",
      "value": " 346M/346M [00:23&lt;00:00, 15.0MB/s]"
     }
    },
    "892ab87763ca4569b1c14c7a864f0327": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1de61920e64e41c8ba8da72e7ef9e27d",
      "max": 160,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_598d0a9ff37e4051a77fe32e33c89d69",
      "value": 160
     }
    },
    "a6023bb8327044c4a6b478656261ef7d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6a703307fa84008a1a651a0bde9aff9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf4f03a541d94ab29d4825910471cf27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d54bd61599c740ce9afbd6dd0cdf3918": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_892ab87763ca4569b1c14c7a864f0327",
       "IPY_MODEL_f9ccf45eaeae49cd9ab55c8524fd06f7"
      ],
      "layout": "IPY_MODEL_6373a61866e8427899a8c56ac463fc88"
     }
    },
    "d7ae2eb9cdf24069ad682092497e0bfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f60180a0cd674bb2b8842f6f174613b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6715a58a5d564ebf8bf64b705cf6f76c",
      "max": 345636463,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cf4f03a541d94ab29d4825910471cf27",
      "value": 345636463
     }
    },
    "f69783a89ebf4887aa55388c5f936d3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9ccf45eaeae49cd9ab55c8524fd06f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6023bb8327044c4a6b478656261ef7d",
      "placeholder": "​",
      "style": "IPY_MODEL_f69783a89ebf4887aa55388c5f936d3b",
      "value": " 160/160 [00:07&lt;00:00, 20.7B/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
